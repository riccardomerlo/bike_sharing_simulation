# -*- coding: utf-8 -*-
"""4_simulation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aRP04RguJFnsUN2_66wSNZNidbKpUGZE

# Libraries
"""

from sklearn.neighbors import KernelDensity
from datetime import datetime, timedelta
import calendar
from collections import Counter
import pandas as pd
import numpy as np
import random
import json
import pickle
import sys
import os
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(rc={'figure.figsize': (20, 8)})

# statistical


base_path = ''

"""# Functions"""


def transform_data(df):
    df = df.copy().dropna()

    df['started_at_round'] = pd.to_datetime(df['time1'], errors='coerce')
    df['ended_at_round'] = pd.to_datetime(df['time2'], errors='coerce')
    df['start'] = df['started_at_round'].dt.strftime('%H:%M')
    df['end'] = df['ended_at_round'].dt.strftime('%H:%M')

    df['month'] = df['started_at_round'].dt.month
    df['week'] = df['started_at_round'].dt.isocalendar().week
    df['weekday'] = df['started_at_round'].dt.dayofweek

    df['season'] = df['started_at_round'].dt.month % 12 // 3 + \
        1  # le raggruppa a inizio mese, non nella data precisa
    df['weekend'] = df['started_at_round'].dt.dayofweek > 4
    df['hour_group'] = regroup_values(list(df['started_at_round'].dt.hour), 3)
    return df


def find_by(df, season, weekend, hour_group, station_start=None, station_end=None, min_limit=2):
    query = df.copy()

    if season:
        query = query[(query['season'] == season)]
    if weekend:
        query = query[(query['weekend'] == weekend)]
    if hour_group:
        query = query[(query['hour_group'] == hour_group)]

    if station_start:
        query = query[query['id1'] == station_start]
    if station_end:
        query = query[query['id2'] == station_end]

    if (not station_start) and (not station_end):
        start_stat_id = Counter(query['id1'])
        stat_ids = [x for x in start_stat_id if start_stat_id[x] > min_limit]
        query = query[query['id1'].isin(stat_ids)]

    return query


def find_from(df, from_start, to_start, weekday=None, month=None, station_start=None, station_end=None, min_limit=2):
    query = df.copy()

    query = query.set_index('started_at_round')

    if weekday:
        query = query[(query['weekday'] == weekday)]
    if month:
        query = query[(query['month'] == month)]

    query = query.between_time(from_start, to_start)

    if station_start:
        query = query[query['id1'] == station_start]
    if station_end:
        query = query[query['id2'] == station_end]

    if (not station_start) and (not station_end):
        start_stat_id = Counter(query['id1'])
        stat_ids = [x for x in start_stat_id if start_stat_id[x] > min_limit]
        query = query[query['id1'].isin(stat_ids)]

    query = query.reset_index()
    return query


def convert_hour_to_float(x):
    hour, minutes = x.split(':')
    def conv(x): return x + (x * (2/3))
    return float(hour) + (conv(float(minutes)/100))


def normalize_probabilities(data):
    """
    takes list of probabilities
    returns list of probabilities which sums to 1
    """
    return data / np.sum(data)


def station_probability(df, start_station, end_station, gen_update, season=None, weekend=None, hour_group=None):
    data = np.expand_dims([convert_hour_to_float(x) for x in find_by(
        df, season, weekend, hour_group, start_station, end_station, 0)['start'].values], 1)

    if len(data) == 0:
        return None

    # ticks
    start = 0
    end = 24-convert_hour_to_float('00:'+str(gen_update))
    freq = int(24 * 60/gen_update)
    x = np.expand_dims(np.linspace(start, end, freq), 1)

    # Do kernel density estimation
    kd = KernelDensity(kernel='gaussian', bandwidth=1).fit(data)
    kd_vals = np.exp(kd.score_samples(x))

    kd_vals_norm = normalize_probabilities(kd_vals)

    # Plot the estimated density
    # plt.plot(x, kd_vals, label='og')
    # plt.plot(x, kd_vals_norm, label='norm')
    # plt.legend()
    # plt.show()

    return {float(x_i): kd_vals_norm[i] for i, x_i in enumerate(x)}


def get_duration_feasibility(df, start, weekday, month, station_start, station_end, tick=5):
    """
    get probabilities of how feasible the trip from A to B is (for each tick) 
    """
    data = np.expand_dims(find_by(df, start, weekday, month,
                                  station_start, station_end)['duration'].values, 1)

    if len(data) == 0:  # never seen trip
        return None

    # per spostare un po' a destra (outliers fino a 150%)
    right_max = max(data) + np.mean(data)

    right_max_rounded = int(tick * np.ceil(np.floor(right_max) / tick))

    # SMOOTH FACTOR: aumenta per avere piu' punti (se serve avere tick piu' piccoli) e.g. 5 tick -> rate 2 = 2.5 tick
    tick_rate = 1
    distance_tick = int((right_max_rounded / tick) * tick_rate) + 1

    # ticks
    x = np.expand_dims(np.linspace(0, right_max_rounded, distance_tick), 1)

    # Do kernel density estimation
    kd = KernelDensity(kernel='gaussian', bandwidth=1).fit(data)
    kd_vals = np.exp(kd.score_samples(x))

    kd_vals_norm = normalize_probabilities(kd_vals)

    # Plot the estimated density
    #plt.plot(x, kd_vals)
    # plt.show()

    return {int(x_i): kd_vals_norm[i] for i, x_i in enumerate(x)}


def get_time(start, gen_update):

    hour, minutes = start.split(':')
    hour = int(hour)
    minutes = int(minutes)
    to_hour = int(np.floor(gen_update / 60))
    to_minutes = gen_update % 60

    new_hour = hour + to_hour
    new_minutes = minutes + to_minutes

    if (new_minutes % 60 == 0):
        new_hour += 1
        new_minutes = 0

    return str(new_hour) + ':' + str(new_minutes)


def generate_outgoing_bikes(df, station_start, _datetime, gen_update):
    """
    generate random number of bikes leaving a station in a given tuple (HH:MM, weekday, month)
    using probability distribution over observed data in previous scenarios
    """
    start = _datetime.strftime('%H:%M'),
    weekday = _datetime.weekday(),
    month = _datetime.month

    # NON HO IDEA DEL PERCHE' MA ALL'IMPROVVISO START E WEEKDAY SONO TUPLE DI UN ELEMENTO
    start = start[0]
    weekday = weekday[0]

    to_start = get_time(start, gen_update)
    query = find_from(df, start, to_start, weekday,
                      month, station_start.id, None)

    if len(query) == 0:  # no data available
        return 0  # assuming that if didn't happen in two entire years, it will never happen

    """
  need to add zeros for weeks not present in data = days when no bikes where used
  otherwise I will only look at weeks in which at least a bike left the station, 
  and that does not reflect reality
  """
    years = set(query['started_at_round'].dt.year)

    #print(month, weekday, years)
    total_weeks = int(np.sum([len([1 for i in calendar.monthcalendar(
        y, month) if i[weekday] != 0]) for y in years]))
    query_data = list(query.groupby('week').count()['bike_id'].values)

    data = query_data + [0 for x in range(total_weeks-len(query_data))]
    data = np.expand_dims(data, 1)
    # per spostare un po' a destra (outliers fino a 150%)
    right_max = max(data) + np.mean(data)

    # Plot the true distribution
    x = np.expand_dims(np.linspace(
        0, int(right_max), int(np.ceil(right_max))), 1)

    # Do kernel density estimation
    kd = KernelDensity(kernel='gaussian', bandwidth=1).fit(data)
    kd_vals = np.exp(kd.score_samples(x))

    #plt.plot(x, kd_vals)
    # plt.show()
    get_prob = {int(x_i): kd_vals[i] for i, x_i in enumerate(x)}

    # print(get_prob)
    # print(normalize_probabilities(list(get_prob.values())))

    n_bikes = np.random.choice(
        list(get_prob.keys()), p=normalize_probabilities(list(get_prob.values())))
    if n_bikes > len(station_start.available_bikes):
        n_bikes = len(station_start.available_bikes)
    return n_bikes

# # DIZIONARIO PRE-SALVATO CON TUTTE LE STOPPING PROBABILITIES
# d = {}

# for season in set(df['season']):
#   tmp1 = df[df['season']==season].copy()
#   d[season] = {}

#   for weekend in set(tmp1['weekend']):
#     tmp2 = tmp1[tmp1['weekend']==weekend].copy()
#     d[season][weekend] = {}

#     for hour_group in set(tmp2['hour_group']):
#       tmp3 = tmp2[tmp2['hour_group']==hour_group].copy()

#       data = [1-len(tmp3[tmp3['duration']>i])/len(tmp3) for i in range(200)]
#       data = [data[0]] + list(np.convolve(data, np.ones(3)/3, mode='valid')) + [data[-1]]

#       d[season][weekend][hour_group] = {i: data[i] for i in range(200)}

# with open(base_path+'data/stopping_probability_dict.pkl', 'wb') as fp:
#     pickle.dump(d, fp)


def filter_prob(station_probs, threshold=3/4):

    # normalizzo probabilita'

    probs = list(station_probs.values())

    station_probs_filter = {station: probs[i] for i, station in enumerate(
        station_probs) if probs[i] > np.quantile(probs, threshold)}

    drive_prob = 1 - np.sum(list(station_probs_filter.values()))

    """
  problema DRIVE , come calcolo probabilita'?
  """

    # station_probs_filter['DRIVE'] = drive_prob

    probs_norm = normalize_probabilities(list(station_probs_filter.values()))

    return {str(station): probs_norm[i] for i, station in enumerate(station_probs_filter)}

# into intervals of size n


def regroup_values(values, n):
    return [n*(h//n) for h in values]


"""### Classes definition"""


class Simulation:

    def __init__(self, df, start_time, end_time, gen_update, station_ids, total_bikes):
        self.df = df
        self.gen_update = gen_update
        self.start_time = start_time
        self.end_time = end_time
        self.cur_time = start_time
        self.cur_generation = 0
        self.station_ids = station_ids
        self.running_bikes = []
        dock_sizes = pd.read_csv(
            base_path+'data/stats/san_jose/max_parked_bikes_by_station.csv').set_index('id').to_dict()['max_parked_bikes']
        n_starting_bikes = {k: int(
            total_bikes*v/np.sum(list(dock_sizes.values()))) for k, v in dock_sizes.items()}
        self.stations = [
            Station(_id, dock_sizes[_id], n_starting_bikes[_id]) for _id in station_ids]

    def simulate(self):
        while self.cur_time < self.end_time:
            print('\n----------------------------------------')
            print('START GENERATION', self.cur_generation,
                  '(' + str(self.cur_time) + ')')
            self.update()
            self.add()
            self.cur_time += timedelta(minutes=self.gen_update)
            self.cur_generation += 1
        self.save_data()

    def save_data(self):
        tot = [bike.history for bike in self.running_bikes]
        for station in self.stations:
            tot += [bike.history for bike in station.available_bikes]
        with open(base_path+'data/simulation.pkl', 'wb') as fp:
            pickle.dump(tot, fp)

    def update(self):
        """
        for each bike, computes next state prediction
        """
        # remove bikes if arrived
        self.running_bikes = [
            bike for bike in self.running_bikes if bike.is_running]
        print('# of running bikes:', len(self.running_bikes))

        for bike in self.running_bikes:
            print()

            # update drive time
            bike.drive_time = (self.cur_generation -
                               bike.start_gen) * self.gen_update
            print(bike)

            # probability of not stopping
            tmp = df[(df['season'] == bike.get_start_season()) & (
                df['weekend'] == bike.get_start_weekend()) & (df['hour_group'] == bike.get_start_hour_group())]
            stop_prob = stop_prob_dict[bike.get_start_season(
            )][bike.get_start_weekend()][bike.get_start_hour_group()][bike.drive_time]
            r = random.random()
            print('       stopping probability:',
                  stop_prob, '| stops?', r <= stop_prob)
            if r > stop_prob:  # doesn't stop
                continue

            # probability of arrival at each station
            station_probs = self.prob_dest_station(bike)
            station_probs = filter_prob(station_probs, threshold=3/4)
            pred = np.random.choice(list(station_probs.keys()), p=normalize_probabilities(
                list(station_probs.values())))
            print('       stopped in station:', pred,
                  'with probability:', station_probs[pred])

            # bike has arrived
            if pred != 'DRIVE':
                bike.stop(self.cur_time, int(pred))

    def prob_dest_station(self, bike):
        """
        given a bike, computes probability to stop in each station
        """
        stat_probs = {station: 0 for station in (
            self.station_ids - {bike.start_station.id})}

        for station in (self.station_ids - {bike.start_station.id}):
            # per ogni stazione, prob rispetto tempo passato
            pf = get_duration_feasibility(
                self.df, None, None, None, bike.start_station.id, station, self.gen_update)

            if pf and (bike.drive_time in pf):
                # per ogni stazione, prob rispetto alle statistiche passate
                ps = station_probability(
                    self.df, bike.start_station.id, station, self.gen_update)

                if ps:
                    stat_probs[station] = ps[convert_hour_to_float(
                        bike.get_start_hour())] * pf[bike.drive_time]

        return stat_probs

    def add(self):
        """
        generate new outgoing bikes
        """
        gen_table = {
            station: generate_outgoing_bikes(
                self.df,
                station,
                self.cur_time,
                self.gen_update
            ) for station in self.stations
        }
        for station in gen_table:
            out = station.leave(gen_table[station])
            for bike in out:
                bike.start(self.cur_time, self.cur_generation, station)
            self.running_bikes += out


class Station:

    def __init__(self, _id, dock_size, n_starting_bikes):
        self.id = _id
        self.dock_size = dock_size
        self.available_bikes = [Bike(str(_id)+'_'+str(i))
                                for i in range(n_starting_bikes)]

    def leave(self, n):
        random.shuffle(self.available_bikes)
        tmp = self.available_bikes[:n].copy()
        self.available_bikes = self.available_bikes[n:].copy()
        return tmp

    def __str__(self):
        return '[STATION] ' + str(self.__dict__)

    def __repr__(self):
        return self.__dict__


class Bike:

    def __init__(self, _id):
        self.id = _id
        self.start_station = None
        self.start_time = None
        self.start_gen = None
        self.drive_time = 0
        self.is_running = False
        self.history = []

    def start(self, t, gen, start_station):
        self.start_time = t
        self.start_gen = gen
        self.start_station = start_station
        self.is_running = True

    def stop(self, t, end_station):
        self.history.append({
            'start_time': self.start_time,
            'start_station': self.start_station,
            'end_time': t,
            'stop_station': end_station
        })
        self.start_station = None
        self.start_time = None
        self.start_gen = None
        self.is_running = False
        self.drive_time = 0

    def get_start_hour(self):
        return self.start_time.strftime('%H:%M')

    def get_start_weekday(self):
        return self.start_time.weekday()

    def get_start_month(self):
        return self.start_time.month

    def get_start_season(self):
        return self.start_time.month % 12 // 3 + 1

    def get_start_weekend(self):
        return self.start_time.weekday() > 4

    def get_start_hour_group(self):
        return regroup_values([self.start_time.hour], 3)[0]

    def __str__(self):
        x = {'start_station': self.start_station.id, 'start_time': self.start_time,
             'start_gen': self.start_gen, 'drive_time': self.drive_time}
        return '[BIKE] ' + str(x)

    def __repr__(self):
        return self.__dict__


"""# Read file"""

df = pd.read_csv(base_path+'preprocessed_data_san_jose.csv')

df = transform_data(df)

"""# Simulation

### Parameters
"""

# # PARAMETERS
STATIONS = set(df['id1']).union(set(df['id2']))
# GEN_UPDATE = 5 # frequency of update in minutes
# TOTAL_BIKES = 450
# start_date = datetime(2022, 5, 3, 9, 0, 0)
# end_date = datetime(2022, 5, 3, 10, 0, 0)

with open(base_path+'stopping_probability_dict.pkl', 'rb') as fp:
    stop_prob_dict = pickle.load(fp)


# """### Run"""

# sim = Simulation(df, start_date, end_date, GEN_UPDATE, STATIONS, TOTAL_BIKES)

# sim.simulate()


"""### Create Precomputed Data"""

# per farne 5 ci ha messo 2 ore e 15

already_done = set([int(x[:x.find('_')])
                    for x in os.listdir(base_path+'data/probabilities')])
already_done

# SALVATAGGIO DATI
GEN_UPDATE = 5

for st1 in STATIONS-already_done:
    print('\n---------', st1)
    d = {}

    for st2 in STATIONS-{st1}:
        print(st2, end=', ')
        d[st2] = {}

        for season in [1, 2, 3, 4]:
            d[st2][season] = {}

            for weekend in [True, False]:
                d[st2][season][weekend] = {}

                for hour_group in set(df['hour_group']):

                    pf = get_duration_feasibility(
                        df, season, weekend, hour_group, st1, st2, GEN_UPDATE)
                    ps = station_probability(
                        df, st1, st2, GEN_UPDATE, season, weekend, hour_group)

                    d[st2][season][weekend][hour_group] = {'pf': pf, 'ps': ps}

    with open(base_path+'data/probabilities/'+str(st1)+'_probabilities.pkl', 'wb') as fp:
        pickle.dump(d, fp)
